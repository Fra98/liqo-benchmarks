consumer:
  version: rancher/k3s:v1.25.0-k3s1

  resources:
    limits:
      cpu: 2000m  #8000m
      memory: 4096Mi #32768Mi
    requests:
      cpu: 2000m  #8000m
      memory: 4096Mi #32768Mi

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - bench-cluster1-worker  #worker-1


provider:
  version: rancher/k3s:v1.25.0-k3s1

  resources:
    limits:
      cpu: 2000m  #8000m
      memory: 4096Mi #32768Mi
    requests:
      cpu: 2000m  #8000m
      memory: 4096Mi #32768Mi

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - bench-cluster1-worker2 #worker-2


hollow:
  # version: giorio94/kubemark:v1.21.4-custom
  # kubectlImage: bitnami/kubectl:1.21
  version: fra98/kubemark:v1.25.0-patched
  kubectlImage: bitnami/kubectl:1.25  

  replicaCount: 1

  kubeletResources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 128Mi

  proxyResources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: NotIn
            values:
            - bench-cluster1-worker    #worker-1
            - bench-cluster1-worker2   #worker-2


settings:
  proxyRegistry: http://docker-registry:5000
  memTotalGB: 32


liqo:
  install: true

  repo: https://helm.liqo.io/
  chart: liqo
  version: "v0.8.3"

  namespace: liqo

  peer:
    image: fra98/liqo-peering-measurer:v0.8.3
    consumer: false   #true
    provider: false   #false

  extraLatency:
    enabled: false
    image: giorio94/tc:0.1
    delay: 100ms

  values: |
    tag: ""

    controllerManager:
      config:
        resourceSharingPercentage: 100
      pod:
        extraArgs:
          - --v=4
          - --kubelet-cpu-limits=8000m
          - --kubelet-cpu-requests=25m
          - --kubelet-ram-limits=8192M
          - --kubelet-ram-requests=50M

    networkManager:
      config:
        podCIDR: "10.42.0.0/16"
        serviceCIDR: "10.43.0.0/16"
        reservedSubnets:
        - 192.168.0.0/16
        - 172.16.0.0/16
        - 10.96.0.0/12
      pod:
        extraArgs:
          - --v=4

    gateway:
      service:
        type: NodePort

    auth:
      config:
        enableAuthentication: false
      service:
        type: LoadBalancer
      pod:
        extraArgs:
          - --v=4

    discovery:
      config:
        clusterName: __CLUSTER_NAME__
        enableAdvertisement: false
        enableDiscovery: false

    crdReplicator:
      pod:
        extraArgs:
          - --v=4

    metricAgent:
        enable: false

    telemetry:
      enable: false

admiralty:
  install: false

  repo: https://charts.admiralty.io
  chart: multicluster-scheduler
  namespace: admiralty
  version: 0.14.1

  certmanager:
    repo: https://charts.jetstack.io
    chart: cert-manager
    namespace: cert-manager
    version: v0.16.1


tensile:
  install: false

  certmanager:
    repo: https://charts.jetstack.io
    chart: cert-manager
    namespace: cert-manager
    version: v1.6.1
